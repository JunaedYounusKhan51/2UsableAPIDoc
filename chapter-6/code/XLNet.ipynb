{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1MrKns_gZv_k_MLy2y_KP1qUfwJzD9AQJ","timestamp":1623219389927}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"MpxvQ14HFYM1"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jGuiOxBdFh3k"},"source":["!pip install -q keras"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"utYxkMR9F0wq"},"source":["!pip install -q pydrive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OIJwvszjF4AZ"},"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HRjYe1E_DNdu"},"source":["project_path = '/content/drive/My Drive/Code Documentation Project/'## we will store our data in this drive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ollFx-9CU-D"},"source":["!pip install simpletransformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pdPi6dlzJ-Pe"},"source":["import re\n","import string\n","import numpy as np\n","import pandas as pd\n","\n","\n","def clean_text(text):\n","\n","    text = re.sub(r\"\\n\", \" \", text)\n","    text = re.sub(r\"\\r\", \" \", text)\n","\n","\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iFs-EunepF9l"},"source":["#### MY MY MY\n","#!pip install -q iterative-stratification\n","\n","from sklearn.model_selection import train_test_split\n","from simpletransformers.classification import MultiLabelClassificationModel\n","\n","import glob\n","\n","import os\n","#from sklearn.model_selection import KFold\n","#from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","import gc\n","import keras.backend as K\n","import numpy as np\n","import pickle\n","import pandas as pd\n","\n","\n","\n","##########\n","##########\n","############ Change File Name HERE\n","\n","#file_name_to_evaluate = 'docsmell_extension_sample_1_junaed.xlsx'\n","\n","#main_df = pd.read_excel(project_path + 'Documentation Smell (Extension)/Dataset/'+file_name_to_evaluate)\n","\n","#path = r'C:\\DRO\\DCL_rawdata_files' # use your path\n","\n","all_files = glob.glob(project_path+\"Documentation Smell (Extension)/Dataset/all labelled sample sets-extension/\" + \"/*.xlsx\")\n","#all_files = glob.glob(project_path+\"Documentation Smell (Extension)/Dataset/saner full dataset/\" + \"/*.xlsx\")\n","\n","\n","main_df = pd.DataFrame()\n","\n","for filename in all_files:\n","    df = pd.read_excel(filename)\n","    try:\n","      df['Documentation Text'] = df['Documentation Text'].apply(clean_text)\n","    except:\n","      print(filename)\n","      pass\n","    main_df = main_df.append(df,ignore_index=True)\n","\n","del all_files\n","del df\n","\n","main_df = main_df.dropna(axis=1, how='any') #deleting (auto-generated) extra columns (containing all NaN values\n","\n","selected_columns = ['Id',\t'Method Prototype',\t'Documentation Text',\t'Fragmented',\t'Tangled',\t'Excessive Structured',\t'Bloated',\t'Lazy']\n","\n","main_df = main_df[selected_columns]\n","\n","\n","saner_df = pd.read_excel(project_path + 'Documentation Smell (Extension)/Dataset/labelled_dataset_full_SANER.xlsx')\n","main_df = main_df.append(saner_df,ignore_index=True)\n","\n","#main_df = main_df.head(n=50) ### to be commented out\n","\n","\n","del saner_df\n","\n","print(len(main_df))\n","\n","print(main_df)\n","\n","#main_df = main_df.head(1000) #### TO BE COMMENTED OUT\n","\n","\n","#main_df = main_df.sample(n = 100) #### TO BE COMMENTED OUT\n","#main_df = main_df.reset_index(drop=True) #### TO BE COMMENTED OUT\n","\n","\n","\n","\n","\n","#X_train_all = np.array(text)\n","\n","#print(X_train_all)\n","#print(y_train_all)\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IeaH7rrAJ8qv"},"source":["#from DataPrep.Clean_Texts import clean_text\n","import pickle\n","#dataset=dataset.head(10)\n","\n","text=main_df['Documentation Text']\n","text=text.map(lambda x: clean_text(x))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SkmPWG1O2MtI"},"source":["print(text[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dhLPBSV92jQw"},"source":["label=main_df.iloc[:,3:8].values\n","\n","print(label[:5,:])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"StWIEAlJ9ZRW"},"source":["def accuracy_multilabel(y_true,y_pred):\n","    cnt=0\n","\n","    N=len(y_pred[0])\n","    if(N==0):\n","        return 0\n","\n","    total=len(y_true)\n","\n","    for i in range(total):\n","        cnt_temp=0\n","        for j in range(N):\n","            if(y_true[i][j]==y_pred[i][j]):\n","                cnt_temp+=1\n","        cnt+=cnt_temp\n","\n","    cnt/=N\n","    acc=cnt/total\n","\n","    return acc\n","\n","\n","\n","def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n","    '''\n","    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n","    http://stackoverflow.com/q/32239577/395857\n","    '''\n","    y_true = np.array(y_true)\n","    y_pred = np.array(y_pred)\n","\n","    acc_list = []\n","    for i in range(y_true.shape[0]):\n","        set_true = set( np.where(y_true[i])[0] )\n","        set_pred = set( np.where(y_pred[i])[0] )\n","        #print('\\nset_true: {0}'.format(set_true))\n","        #print('set_pred: {0}'.format(set_pred))\n","        tmp_a = None\n","        if len(set_true) == 0 and len(set_pred) == 0:\n","            tmp_a = 1\n","        else:\n","            tmp_a = len(set_true.intersection(set_pred))/\\\n","                    float( len(set_true.union(set_pred)) )\n","        #print('tmp_a: {0}'.format(tmp_a))\n","        acc_list.append(tmp_a)\n","    return np.mean(acc_list)\n","\n","\n","def exact_match(y_true,y_pred):\n","\n","    N=len(y_pred[0])\n","    if(N==0):\n","        return 0\n","\n","    total=0\n","    accurate = 0\n","\n","    for i in range(len(y_pred)):\n","        for j in range(N):\n","            if(y_true[i][j]==y_pred[i][j]):\n","                accurate+=1\n","            total+=1\n","\n","    acc=accurate/total\n","\n","    return acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0G0v_CtqUJ_w"},"source":["main_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oI6rtRZ8DEwp"},"source":["model_type = \"xlnet\"\n","model_name = \"xlnet-base-cased\"\n","\n","train_args = {\n","    \"reprocess_input_data\": True,\n","    \"overwrite_output_dir\": True,\n","    \"use_cached_eval_features\": True,\n","    \"output_dir\": project_path + \"Models/Transformers/output/\"+model_type,\n","    \"best_model_dir\": project_path + \"Models/Transformers/output/\"+model_type+\"/best_model\",\n","    \"use_early_stopping\": False,\n","    \"early_stopping_delta\": 0.0,\n","    \"early_stopping_metric\": \"eval_loss\",\n","    \"early_stopping_metric_minimize\" : True,\n","    \"early_stopping_patience\" : 2,\n","    \"evaluate_during_training\": True,\n","    \"evaluate_during_training_steps\": 512,\n","    \"wandb_project\": \"Simple Sweep\",\n","    \"wandb_kwargs\": {\"name\": model_name},\n","    \"save_model_every_epoch\": False,\n","    \"save_eval_checkpoints\": False,\n","    \"evaluate_during_training_verbose\" : True,\n","    \"num_train_epochs\": 10\n","}\n","\n","train_args[\"max_seq_length\"] = 300\n","train_args[\"train_batch_size\"] = 16\n","train_args[\"gradient_accumulation_steps\"] = 2\n","train_args[\"evaluate_during_training\"] = False\n","train_args[\"evaluate_during_training_steps\"] = 25\n","train_args[\"use_early_stopping\"] = False\n","train_args[\"learning_rate\"] = 5e-5\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-TmSJDTElHv"},"source":["ll=[]\n","ll.append([text[1],label[1]])\n","print(ll)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DBvFUsR0iDSG"},"source":["\n","\n","!pip install -q iterative-stratification\n","\n","from sklearn.model_selection import train_test_split\n","from simpletransformers.classification import MultiLabelClassificationModel\n","\n","import glob\n","\n","import os\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","import gc\n","import keras.backend as K\n","import numpy as np\n","\n","from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n","\n","\n","from sklearn.metrics import classification_report,precision_recall_fscore_support\n","from sklearn.metrics import precision_score,recall_score,f1_score\n","from sklearn.metrics import accuracy_score, hamming_loss\n","\n","\n","\n","\n","X_train_all = np.array(text)\n","y_train_all = np.array(label)\n","\n","print(X_train_all.shape)\n","print(y_train_all.shape)\n","\n","num_cross_validation = 5\n","\n","\n","mskf = MultilabelStratifiedShuffleSplit(n_splits = num_cross_validation,test_size=0.2, random_state=42)\n","\n","pred_list=[]\n","cvscores = []\n","y_true_all_fold = []\n","pred_binary_all_fold = []\n","\n","Fold_Index = 1\n","\n","for train, val in mskf.split(X_train_all, y_train_all):\n","    gc.collect()\n","    K.clear_session()\n","    print('Fold: ', Fold_Index)\n","    Fold_Index = Fold_Index + 1\n","\n","    X_train = X_train_all[train]\n","    X_val = X_train_all[val]\n","    y_train = y_train_all[train]\n","    y_val = y_train_all[val]\n","\n","    ll=[]\n","    for i in range(len(X_train)):\n","      ll.append([X_train[i],y_train[i]])\n","\n","    train_val_df = pd.DataFrame(ll)\n","    train_val_df.columns = [\"text\", \"labels\"]\n","\n","    ll=[]\n","    for i in range(len(X_val)):\n","      ll.append([X_val[i],y_val[i]])\n","\n","    test_df = pd.DataFrame(ll)\n","    test_df.columns = [\"text\", \"labels\"]\n","\n","\n","\n","    train_df, val_df = train_test_split(train_val_df, test_size=0.1)\n","\n","\n","    # Create a ClassificationModel\n","    model = MultiLabelClassificationModel(model_type, model_name, num_labels=5, args=train_args)\n","\n","    # Train the model\n","    model.train_model(train_df, eval_df=val_df)\n","\n","    pred, raw_outputs = model.predict(test_df['text'].to_list())\n","\n","    pred_binary=np.array(pred)\n","    for i in range(len(pred_binary)):\n","      for j in range(len(pred_binary[i])):\n","        pred_binary[i][j]=int(1*(pred_binary[i][j]>0.5))\n","      y_true_all_fold.append(y_val[i])\n","      pred_binary_all_fold.append(pred_binary[i])\n","\n","    #report=classification_report(y_true_all_fold,pred_binary_all_fold)\n","\n","\n","\n","y_true_np = np.array(y_true_all_fold)\n","y_pred_np = np.array(pred_binary_all_fold)\n","\n","#print(y_true_np)\n","\n","categories_list=['Fragmented','Tangled','Excessive Structured','Bloated','Lazy']\n","\n","# for i in range(len(categories_list)):\n","#   print('Class: ' + categories_list[i])\n","#   new_y_true = y_true_np[:,i]\n","#   new_y_pred = y_pred_np[:,i]\n","#   print(new_y_pred)\n","#   print('Accuracy for class ' + categories_list[id] + ': ' + str(accuracy_score(new_y_true,new_y_pred)))\n","#   print('Classification Report for class ' + categories_list[i] + ': ' + str(classification_report(new_y_true,new_y_pred)))\n","\n","num_classes = 5\n","for i in range(num_classes):\n","  print('Class: ' + str(i))\n","  new_y_true = y_true_np[:,i]\n","  new_y_pred = y_pred_np[:,i]\n","\n","  print('Accuracy for class ' + str(i) + ': ' + str(accuracy_score(new_y_true,new_y_pred)))\n","  print('Classification Report for class' + str(i) + ': ' + str(classification_report(new_y_true,new_y_pred)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9HLfsfcsfi_n"},"source":["from sklearn.metrics import classification_report,precision_recall_fscore_support\n","from sklearn.metrics import precision_score,recall_score,f1_score\n","from sklearn.metrics import accuracy_score, hamming_loss\n","\n","report=classification_report(y_true_np,y_pred_np)\n","#report=precision_recall_fscore_support(y_test,pred_binary,average='micro')\n","print('Classification Report: '+str(report))\n","\n","\n","precision=precision_score(y_true_np,y_pred_np,average='weighted')\n","print('Weighted Precision: '+str(precision))\n","\n","recall=recall_score(y_true_np,y_pred_np,average='weighted')\n","print('Weighted Recall: '+str(recall))\n","\n","f1=f1_score(y_true_np,y_pred_np,average='weighted')\n","print('Weighted F1 Score: '+str(f1))\n","\n","\n","\n","acc_hard=accuracy_score(y_true_np,y_pred_np)\n","print('Hard Accuracy: '+str(acc_hard))\n","\n","#from Custom_Metrics import accuracy_multilabel\n","\n","acc_custom=accuracy_multilabel(y_true_np,y_pred_np)\n","print('Custom Accuracy: '+str(acc_custom))\n","\n","acc_hamming=hamming_score(y_true_np,y_pred_np)\n","print('Hamming Score: '+str(acc_hamming))\n","\n","loss_hamming=hamming_loss(y_true_np,y_pred_np)\n","print('Hamming Loss: '+str(loss_hamming))\n","\n","acc_exact=exact_match(y_true_np,y_pred_np)\n","print('Exact Match: '+str(acc_exact))"],"execution_count":null,"outputs":[]}]}